@article{cheng2022improve,
 abstract = {In forestry studies, deep learning models have achieved excellent performance in many application scenarios (e.g., detecting forest damage). However, the unclear model decisions (i.e., black-box) undermine the credibility of the results and hinder their practicality. This study intends to obtain explanations of such models through the use of explainable artificial intelligence methods, and then use feature unlearning methods to improve their performance, which is the first such attempt in the field of forestry. Results of three experiments show that the model training can be guided by expertise to gain specific knowledge, which is reflected by explanations. For all three experiments based on synthetic and real leaf images, the improvement of models is quantified in the classification accuracy (up to 4.6%) and three indicators of explanation assessment (i.e., root-mean-square error, cosine similarity, and the proportion of important pixels). Besides, the introduced expertise in annotation matrix form was automatically created in all experiments. This study emphasizes that studies of deep learning in forestry should not only pursue model performance (e.g., higher classification accuracy) but also focus on the explanations and try to improve models according to the expertise.},
 author = {Cheng, Ximeng and Doosthosseini, Ali and Kunkel, Julian},
 doi = {10.3389/fpls.2022.902105},
 issn = {1664-462X},
 journal = {Frontiers in Plant Science},
 title = {Improve the Deep Learning Models in Forestry Based on Explanations and Expertise},
 url = {https://www.frontiersin.org/article/10.3389/fpls.2022.902105},
 volume = {13},
 year = {2022}
}

